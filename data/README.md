## Steps

The original dataset was downloaded from [Project Gutenberg](https://www.gutenberg.org/) [here](https://www.gutenberg.org/ebooks/74).
After the corpus was collected and converted to plain text, it was tokenized using the [NLTK's](http://www.nltk.org) implementation of the Punkt sentence tokenizer followed by the Penn Treebank word tokenizer.
This did a margionaly acceptiable job, so the rest was cleaned up my hand.
If any line is not a proper sentence, put in a PR.

